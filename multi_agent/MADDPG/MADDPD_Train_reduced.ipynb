{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradients (MADDPG) - reduced env.\n",
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "no_graphics=True\n",
    "#no_graphics=False\n",
    "env = UnityEnvironment(file_name='C:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab-compet\\Tennis_Windows_x86_64\\Tennis.exe',no_graphics=no_graphics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check time stacking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "   6.83172083  6.         -0.          0.         -6.65278625 -1.55886006\n",
      "  -0.         -0.98100001  6.83172083  5.94114017 -0.         -0.98100001]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -6.4669857  -1.5         0.          0.\n",
      "  -6.83172083  6.          0.          0.         -6.4669857  -1.55886006\n",
      "   0.         -0.98100001 -6.83172083  5.94114017  0.         -0.98100001]]\n",
      "[[-6.65278625 -1.5        -0.          0.          6.83172083  6.\n",
      "  -0.          0.         -6.65278625 -1.55886006 -0.         -0.98100001\n",
      "   6.83172083  5.94114017 -0.         -0.98100001 -6.65278625 -1.71581995\n",
      "  -0.         -1.96200001  6.83172083  5.78418016 -0.         -1.96200001]\n",
      " [-6.4669857  -1.5         0.          0.         -6.83172083  6.\n",
      "   0.          0.         -6.4669857  -1.55886006  0.         -0.98100001\n",
      "  -6.83172083  5.94114017  0.         -0.98100001 -6.4669857  -1.71581995\n",
      "   0.         -1.96200001 -6.83172083  5.78418016  0.         -1.96200001]]\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "print(states)\n",
    "for jj in range(2):\n",
    "  env_info = env.step([[0,0],[0,0]])[brain_name]     # send all actions to the environment\n",
    "  states = env_info.vector_observations         # get next state (for each agent)\n",
    "  print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.65278625 -1.71581995 -0.         -1.96200001  6.83172083  5.78418016\n",
      "  -0.         -1.96200001]\n",
      " [-6.4669857  -1.71581995  0.         -1.96200001 -6.83172083  5.78418016\n",
      "   0.         -1.96200001]]\n",
      "(2, 8)\n",
      "(2, 8)\n",
      "(2, 24)\n",
      "(2, 8)\n"
     ]
    }
   ],
   "source": [
    "d_states = np.zeros((2,8))\n",
    "print(states[:,-8:])\n",
    "print(states[:,-8:].shape)\n",
    "d_states = states[:,-8:]\n",
    "print(d_states.shape)\n",
    "\n",
    "states_f = env_info.vector_observations                  # get the current state (for each agent)\n",
    "print(states_f.shape)\n",
    "states = states_f[:,-8:]\n",
    "print(states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 It's My Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init OUNoise with dim= 2\n",
      "init OUNoise with dim= 2\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from buffer import ReplayBuffer  #\n",
    "from maddpgred import MADDPGRED as MADDPG\n",
    "\n",
    "maddpg = MADDPG()\n",
    "\n",
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def pre_process(entity, batchsize):\n",
    "    processed_entity = []\n",
    "    for j in range(3):\n",
    "        list = []\n",
    "        for i in range(batchsize):\n",
    "            b = entity[i][j]\n",
    "            list.append(b)\n",
    "        c = torch.Tensor(list)\n",
    "        processed_entity.append(c)\n",
    "    return processed_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_ = False\n",
    "\n",
    "scores_window = deque(maxlen=100)  # last 100 max scores\n",
    "scores_window_mean = deque(maxlen=100)  # last 100 mean scores\n",
    "\n",
    "seeding()\n",
    "# number of parallel agents\n",
    "parallel_envs = 1   # start with a single Unity-ML env\n",
    "# number of training episodes.\n",
    "training_episods = 10000*3\n",
    "buffer_length = 100*10000\n",
    "\n",
    "if debug_:\n",
    "    batchsize = 3\n",
    "else:\n",
    "    batchsize = 128*4 \n",
    "\n",
    "UPDATE_EVERY_NTH_STEP = 30\n",
    "UPDATE_MANY_EPOCHS = 10\n",
    "\n",
    "t = 0\n",
    "    \n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "noise = 4  # 2 before 0.1 not enough?\n",
    "noise_reduction =  0.9\n",
    "\n",
    "# how many episodes before update\n",
    "episode_per_update = 2 * parallel_envs\n",
    "\n",
    "log_path = os.getcwd()+\"\\log\"\n",
    "model_dir= os.getcwd()+\"\\model_dir\"\n",
    "    \n",
    "os.makedirs(log_path, exist_ok=True)    \n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "torch.set_num_threads(parallel_envs)\n",
    "    \n",
    "#from tensorboardX import SummaryWriter\n",
    "#logger = SummaryWriter(log_dir=log_path)\n",
    "num_agents = 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 30 30 30 30 30 30 30 30 30 30 23 23 23 30  8 12 14 30 30 30 30 30 30]\n"
     ]
    }
   ],
   "source": [
    "data = np.load('state_scale.npz')\n",
    "scale = data['scale_int']\n",
    "print(scale)\n",
    "scale = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize 512\n",
      "init OUNoise with dim= 2\n",
      "init OUNoise with dim= 2\n",
      "(2, 8)\n",
      "Episode 269\tAverage <Score>: -0.00\tAverage Max Score: 0.00\tMax Score: 0.10\tMax Average Max Score: 0.01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab_compet_MADDPG_reduced\\utilities.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  make_tensor = lambda x: torch.tensor(x, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tAverage <Score>: -0.00\tAverage Max Score: 0.00\tMax Score: 0.10\tMax Average Max Score: 0.01\n",
      "Episode 2000\tAverage <Score>: -0.00\tAverage Max Score: 0.00\tMax Score: 0.00\tMax Average Max Score: 0.01\n",
      "Episode 3000\tAverage <Score>: -0.00\tAverage Max Score: 0.00\tMax Score: 0.00\tMax Average Max Score: 0.06\n",
      "Episode 4000\tAverage <Score>: -0.00\tAverage Max Score: 0.00\tMax Score: 0.00\tMax Average Max Score: 0.06\n",
      "Episode 5000\tAverage <Score>: -0.00\tAverage Max Score: 0.00\tMax Score: 0.10\tMax Average Max Score: 0.06\n",
      "Episode 6000\tAverage <Score>: 0.01\tAverage Max Score: 0.02\tMax Score: 0.10\tMax Average Max Score: 0.066\n",
      "Episode 7000\tAverage <Score>: 0.04\tAverage Max Score: 0.06\tMax Score: 0.30\tMax Average Max Score: 0.144\n",
      "Episode 8000\tAverage <Score>: 0.28\tAverage Max Score: 0.30\tMax Score: 1.50\tMax Average Max Score: 0.31\n",
      "Episode 8321\tAverage <Score>: 0.45\tAverage Max Score: 0.49\tMax Score: 2.70\tMax Average Max Score: 0.49Episode 8322\tAverage Score: 0.51\tScore: 2.60\n",
      "Assignment -DONE-\n",
      "Episode 9000\tAverage <Score>: 0.05\tAverage Max Score: 0.10\tMax Score: 0.20\tMax Average Max Score: 1.64\n",
      "Episode 9899\tAverage <Score>: 0.06\tAverage Max Score: 0.11\tMax Score: 0.20\tMax Average Max Score: 1.64"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-04e38faeade7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ma_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                     \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#soft update the target network towards the actual networks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab_compet_MADDPG_reduced\\maddpg.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, samples, agent_number, logger)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m#critic loss = batch mean of (y- Q(s,a) from target network)^2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m#y = reward of this timestep + discount * Q(st+1,at+1) from target network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mtarget_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mtarget_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab_compet_MADDPG_reduced\\maddpg.py\u001b[0m in \u001b[0;36mtarget_act\u001b[1;34m(self, obs_all_agents, noise)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtarget_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_all_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;34m\"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mtarget_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mddpg_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mddpg_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaddpg_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_all_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab_compet_MADDPG_reduced\\maddpg.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtarget_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_all_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;34m\"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mtarget_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mddpg_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mddpg_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaddpg_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_all_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab_compet_MADDPG_reduced\\ddpg.py\u001b[0m in \u001b[0;36mtarget_act\u001b[1;34m(self, obs, noise)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtarget_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab_compet_MADDPG_reduced\\networkforall.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# return a 2-dim vector: movement toward (or away from) the net, and jumping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonlin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonlin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mh3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# keep 10000 episodes worth of replay\n",
    "buffer = ReplayBuffer(int(buffer_length))  ## buffer ToDo\n",
    "\n",
    "states = np.zeros((2,8))\n",
    "next_states = np.zeros((2,8))\n",
    "\n",
    "print('batchsize',batchsize)\n",
    "\n",
    "# initialize policy and critic\n",
    "maddpg = MADDPG()\n",
    "agent0_reward = []\n",
    "agent1_reward = []\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states_f = env_info.vector_observations                  # get the current state (for each agent)\n",
    "states = states_f[:,-8:]\n",
    "states = states/scale\n",
    "print(states.shape)\n",
    "actions = maddpg.act(torch.from_numpy(states).unsqueeze(0).float(), noise=noise)\n",
    "actions_array = torch.stack(actions).detach().numpy()\n",
    "\n",
    "env_info = env.step(actions_array.squeeze())[brain_name]           # send all actions to the environment\n",
    "next_states_f = env_info.vector_observations         # get next state (for each agent)\n",
    "next_states = next_states_f[:,-8:]\n",
    "next_states = next_states/scale\n",
    "rewards = env_info.rewards                         # get reward (for each agent)\n",
    "\n",
    "not_yet_shown = True\n",
    "max_100_average_score = -1\n",
    "\n",
    "for i_episode in range(1, training_episods):               # train for training_episods many episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    maddpg.rest_noise()                                    # reset the noise object\n",
    "    \n",
    "    states_f = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    states = states_f[:,-8:]\n",
    "    states = states / scale\n",
    "    \n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)        \n",
    "    jj = 0 \n",
    "    while True:\n",
    "        jj += 1 \n",
    "        actions = maddpg.act(torch.from_numpy(states).unsqueeze(0).float(), noise=noise)\n",
    "        noise *= noise_reduction            \n",
    "        actions_array = torch.stack(actions).detach().numpy().squeeze()\n",
    "        \n",
    "        if debug_ :\n",
    "            print('actions_array type',type(actions_array))\n",
    "            print('actions_array shape',actions_array.shape)\n",
    "        \n",
    "        env_info = env.step(actions_array)[brain_name]     # send all actions to the environment\n",
    "        next_states_f = env_info.vector_observations         # get next state (for each agent)\n",
    "        next_states = next_states_f[:,-8:]\n",
    "        next_states = next_states / scale        \n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        \n",
    "        transition = ([states], [actions_array], [rewards], [next_states], [dones])\n",
    "        buffer.push(transition)\n",
    "        \n",
    "        # update once after every episode_per_update\n",
    "        #if len(buffer) > batchsize*10 and i_episode % episode_per_update < parallel_envs:\n",
    "        if len(buffer) > batchsize*10 and i_episode % UPDATE_EVERY_NTH_STEP == 0:          \n",
    "            for k in range(UPDATE_MANY_EPOCHS):\n",
    "                for a_i in range(2):\n",
    "                    samples = buffer.sample(batchsize)\n",
    "                    maddpg.update(samples, a_i)\n",
    "            maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "                \n",
    "        \n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "\n",
    "    scores_window.append(scores.max())       # save most recent score\n",
    "    scores_window_mean.append(scores.mean())       # save most recent score\n",
    "    if np.mean(scores_window) >= 0.5 and not_yet_shown:\n",
    "                    print('Episode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), scores.max()))\n",
    "                    print('Assignment -DONE-')\n",
    "                    not_yet_shown = False\n",
    "    \n",
    "    if max_100_average_score <  np.mean(scores_window):\n",
    "        max_100_average_score = np.mean(scores_window)\n",
    "    print('\\rEpisode {}\\tAverage <Score>: {:.2f}\\tAverage Max Score: {:.2f}\\tMax Score: {:.2f}\\tMax Average Max Score: {:.2f}'.format(i_episode, np.mean(scores_window_mean), np.mean(scores_window), np.max(scores_window), max_100_average_score), end=\"\")                \n",
    "    if i_episode % 1000 == 0:\n",
    "        print('\\rEpisode {}\\tAverage <Score>: {:.2f}\\tAverage Max Score: {:.2f}\\tMax Score: {:.2f}\\tMax Average Max Score: {:.2f}'.format(i_episode,  np.mean(scores_window_mean), np.mean(scores_window), np.max(scores_window), max_100_average_score))        \n",
    "\n",
    "print('')\n",
    "print('Stop it...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "save_dict_list =[]\n",
    "for i in range(2):\n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                             'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                             'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "                save_dict_list.append(save_dict)\n",
    "torch.save(save_dict_list, \n",
    "                           os.path.join(model_dir, 'Run4_episode-{}.pt'.format(i_episode)))\n",
    "\n",
    "\n",
    "save_dict_list =[]\n",
    "for i in range(2):\n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),                             \n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict()}\n",
    "                save_dict_list.append(save_dict)\n",
    "torch.save(save_dict_list, \n",
    "                           os.path.join(model_dir, 'Run4_reduced_episode-{}.pt'.format(i_episode)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
