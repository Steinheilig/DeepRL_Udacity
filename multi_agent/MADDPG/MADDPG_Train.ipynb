{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradients (MADDPG)\n",
    "# Assignment Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.'\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "no_graphics=True\n",
    "#no_graphics=False\n",
    "env = UnityEnvironment(file_name='C:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab-compet\\Tennis_Windows_x86_64\\Tennis.exe',no_graphics=no_graphics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 It's My Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from buffer import ReplayBuffer  ## REWRITE BUFFER  // Check UTILITIS\n",
    "\n",
    "# rewritten MADDPG to have actor/critic networks of appropriate shapes, \n",
    "# i.e. 24 states and 2 actions per agent\n",
    "from maddpg import MADDPG       \n",
    "\n",
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def pre_process(entity, batchsize):\n",
    "    processed_entity = []\n",
    "    for j in range(3):\n",
    "        list = []\n",
    "        for i in range(batchsize):\n",
    "            b = entity[i][j]\n",
    "            list.append(b)\n",
    "        c = torch.Tensor(list)\n",
    "        processed_entity.append(c)\n",
    "    return processed_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_ = False\n",
    "\n",
    "scores_window = deque(maxlen=100)  # last 100 max scores\n",
    "scores_window_mean = deque(maxlen=100)  # last 100 mean scores\n",
    "\n",
    "seeding()\n",
    "# number of parallel agents\n",
    "parallel_envs = 1   # start with a single Unity-ML env\n",
    "# number of training episodes.\n",
    "training_episods = 10000*3\n",
    "buffer_length = 100*10000\n",
    "\n",
    "if debug_:\n",
    "    batchsize = 3\n",
    "else:\n",
    "    batchsize = 128*4 \n",
    "\n",
    "UPDATE_EVERY_NTH_STEP = 30\n",
    "UPDATE_MANY_EPOCHS = 10\n",
    "\n",
    "t = 0\n",
    "    \n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "noise = 4  # 2 before 0.1 not enough?\n",
    "noise_reduction =  0.9\n",
    "\n",
    "# how many episodes before update\n",
    "episode_per_update = 2 * parallel_envs\n",
    "\n",
    "log_path = os.getcwd()+\"/log\"\n",
    "model_dir= os.getcwd()+\"/model_dir\"\n",
    "    \n",
    "os.makedirs(log_path, exist_ok=True)    \n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "torch.set_num_threads(parallel_envs)\n",
    "    \n",
    "#from tensorboardX import SummaryWriter\n",
    "#logger = SummaryWriter(log_dir=log_path)\n",
    "num_agents = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 30 30 30 30 30 30 30 30 30 30 23 23 23 30  8 12 14 30 30 30 30 30 30]\n"
     ]
    }
   ],
   "source": [
    "data = np.load('state_scale.npz')\n",
    "scale = data['scale_int']\n",
    "print(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize 512\n",
      "init OUNoise with dim= 2\n",
      "init OUNoise with dim= 2\n",
      "Episode 329\tAverage <Score>: 0.00\tAverage Max Score: 0.01\tMax Score: 0.10\tMax Average Max Score: 0.022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab-compet\\utilities.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  make_tensor = lambda x: torch.tensor(x, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tAverage <Score>: -0.00\tAverage Max Score: 0.00\tMax Score: 0.00\tMax Average Max Score: 0.02\n",
      "Episode 2000\tAverage <Score>: -0.00\tAverage Max Score: 0.00\tMax Score: 0.00\tMax Average Max Score: 0.02\n",
      "Episode 3000\tAverage <Score>: 0.03\tAverage Max Score: 0.05\tMax Score: 0.20\tMax Average Max Score: 0.144\n",
      "Episode 4000\tAverage <Score>: 0.12\tAverage Max Score: 0.16\tMax Score: 0.90\tMax Average Max Score: 0.164\n",
      "Episode 5000\tAverage <Score>: 0.15\tAverage Max Score: 0.18\tMax Score: 1.00\tMax Average Max Score: 0.186\n",
      "Episode 5037\tAverage Score: 0.52\tScore: 2.60ax Score: 0.50\tMax Score: 2.70\tMax Average Max Score: 0.50\n",
      "Assignment -DONE-\n",
      "Episode 6000\tAverage <Score>: 0.58\tAverage Max Score: 0.61\tMax Score: 2.70\tMax Average Max Score: 2.50\n",
      "Episode 6239\tAverage <Score>: 1.93\tAverage Max Score: 1.96\tMax Score: 2.70\tMax Average Max Score: 2.50"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-307299cd2349>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ma_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                     \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#soft update the target network towards the actual networks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab-compet\\maddpg.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, samples, agent_number, logger)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mhuber_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSmoothL1Loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhuber_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mcritic_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m## added this clipping... which was uncommented in orgi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m#torch.nn.utils.clip_grad_norm_(agent.critic.parameters(), 0.5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# keep 1e6 samples of replay\n",
    "buffer = ReplayBuffer(int(buffer_length))  #\n",
    "\n",
    "print('batchsize',batchsize)\n",
    "\n",
    "# initialize policy and critic\n",
    "maddpg = MADDPG()\n",
    "agent0_reward = []\n",
    "agent1_reward = []\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "states = states/scale\n",
    "actions = maddpg.act(torch.from_numpy(states).unsqueeze(0).float(), noise=noise)\n",
    "actions_array = torch.stack(actions).detach().numpy()\n",
    "\n",
    "env_info = env.step(actions_array.squeeze())[brain_name]           # send all actions to the environment\n",
    "next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "next_states = next_states/scale\n",
    "rewards = env_info.rewards                         # get reward (for each agent)\n",
    "\n",
    "not_yet_shown = True\n",
    "max_100_average_score = -1\n",
    "\n",
    "for i_episode in range(1, training_episods):               # train for training_episods many episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    maddpg.rest_noise()                                    # reset the noise object\n",
    "    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    states = states / scale\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)        \n",
    "    jj = 0 \n",
    "    while True:\n",
    "        jj += 1 \n",
    "        actions = maddpg.act(torch.from_numpy(states).unsqueeze(0).float(), noise=noise)\n",
    "        noise *= noise_reduction            \n",
    "        actions_array = torch.stack(actions).detach().numpy().squeeze()\n",
    "        \n",
    "        if debug_ :\n",
    "            print('actions_array type',type(actions_array))\n",
    "            print('actions_array shape',actions_array.shape)\n",
    "       \n",
    "        env_info = env.step(actions_array)[brain_name]     # send all actions to the environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        next_states = next_states / scale\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        \n",
    "        transition = ([states], [actions_array], [rewards], [next_states], [dones])\n",
    "        buffer.push(transition)\n",
    "        \n",
    "        # update once after every episode_per_update\n",
    "        #if len(buffer) > batchsize*10 and i_episode % episode_per_update < parallel_envs:\n",
    "        \n",
    "        if len(buffer) > batchsize*10 and i_episode % UPDATE_EVERY_NTH_STEP == 0:          \n",
    "            for k in range(UPDATE_MANY_EPOCHS):\n",
    "                for a_i in range(2):\n",
    "                    samples = buffer.sample(batchsize)\n",
    "                    maddpg.update(samples, a_i)\n",
    "            maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "                \n",
    "        \n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "\n",
    "    scores_window.append(scores.max())       # save most recent score\n",
    "    scores_window_mean.append(scores.mean())       # save most recent score\n",
    "    if np.mean(scores_window) >= 0.5 and not_yet_shown:\n",
    "                    print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), scores.max()))\n",
    "                    print('Assignment -DONE-')\n",
    "                    not_yet_shown = False\n",
    "    \n",
    "    if max_100_average_score <  np.mean(scores_window):\n",
    "        max_100_average_score = np.mean(scores_window)\n",
    "    print('\\rEpisode {}\\tAverage <Score>: {:.2f}\\tAverage Max Score: {:.2f}\\tMax Score: {:.2f}\\tMax Average Max Score: {:.2f}'.format(i_episode, np.mean(scores_window_mean), np.mean(scores_window), np.max(scores_window), max_100_average_score), end=\"\")                \n",
    "    if i_episode % 1000 == 0:\n",
    "        print('\\rEpisode {}\\tAverage <Score>: {:.2f}\\tAverage Max Score: {:.2f}\\tMax Score: {:.2f}\\tMax Average Max Score: {:.2f}'.format(i_episode,  np.mean(scores_window_mean), np.mean(scores_window), np.max(scores_window), max_100_average_score))        \n",
    "\n",
    "            \n",
    "    #print('Score (max over agents) from episode {}: {} , steps: {}'.format(i, np.max(scores),jj))\n",
    "\n",
    "print('')\n",
    "print('Stop it...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "save_dict_list =[]\n",
    "for i in range(2):\n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                             'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                             'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "                save_dict_list.append(save_dict)\n",
    "torch.save(save_dict_list, \n",
    "                           os.path.join(model_dir, 'Run5_episode-{}.pt'.format(i_episode)))\n",
    "\n",
    "\n",
    "save_dict_list =[]\n",
    "for i in range(2):\n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),                             \n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict()}\n",
    "                save_dict_list.append(save_dict)\n",
    "torch.save(save_dict_list, \n",
    "                           os.path.join(model_dir, 'Run5_reduced_episode-{}.pt'.format(i_episode)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging/Checking the Code Elements...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: tensor([[-0.9839,  0.8409],\n",
      "        [-0.9907,  0.9475],\n",
      "        [-0.9735,  0.9821],\n",
      "        ...,\n",
      "        [-0.9914,  0.9628],\n",
      "        [-0.9842,  0.9752],\n",
      "        [-0.9856,  0.9375]], grad_fn=<AddBackward0>)\n",
      "Target Actions: tensor([[-0.9827,  0.8484],\n",
      "        [-0.9895,  0.9705],\n",
      "        [-0.9454,  0.8281],\n",
      "        ...,\n",
      "        [-0.9919,  0.9661],\n",
      "        [-0.9791,  0.9751],\n",
      "        [-0.9901,  0.9651]], grad_fn=<AddBackward0>)\n",
      "Actions: tensor([[0.9859, 0.9818],\n",
      "        [0.9948, 0.9958],\n",
      "        [0.9983, 0.9906],\n",
      "        ...,\n",
      "        [0.9831, 0.9839],\n",
      "        [0.9979, 0.9948],\n",
      "        [0.9819, 0.9631]], grad_fn=<AddBackward0>)\n",
      "Target Actions: tensor([[0.9828, 0.9795],\n",
      "        [0.9950, 0.9903],\n",
      "        [0.9381, 0.9304],\n",
      "        ...,\n",
      "        [0.9814, 0.9835],\n",
      "        [0.9973, 0.9929],\n",
      "        [0.9951, 0.9900]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from utilities import soft_update, transpose_to_tensor, transpose_list\n",
    "samples = buffer.sample(batchsize)\n",
    "obs, action, reward, next_obs, done = map(transpose_to_tensor, samples)\n",
    "actions = maddpg.act(obs)\n",
    "target_actions = maddpg.target_act(next_obs)\n",
    "print('Actions:',actions[0])\n",
    "print('Target Actions:',target_actions[0])\n",
    "print('Actions:',actions[1])\n",
    "print('Target Actions:',target_actions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab-compet\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "#https://realpython.com/python-logging/\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "#logging.basicConfig(filename='C:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab-compet\\test.log', filemode='a', format='%(name)s - %(levelname)s - %(message)s', level=logging.DEBUG)\n",
    "logging.basicConfig(filename='test.txt')\n",
    "print(os.getcwd())\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logger.debug('This will get logged')\n",
    "logger.warning('This will get logged to a file')\n",
    "logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -7.6477, -1.5000, -0.0000,  0.0000,  6.7403,  5.9765, -0.0000,  0.0000,\n",
      "        -6.4677, -1.5589, 11.8004, -0.9810,  6.7403,  5.8587, 11.8004, -0.9810,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -7.7758, -1.5000,  0.0000,  0.0000, -6.7403,  5.9765,  0.0000,  0.0000,\n",
      "        -6.3379, -1.5589, 14.3793, -0.9810, -6.7403,  5.8587, 14.3793, -0.9810])\n",
      "Actions:shape torch.Size([1000, 4])\n",
      "Actions: 704: tensor([ 0.0214,  0.4292, -0.9239, -0.4335], grad_fn=<SliceBackward0>)\n",
      "target_critic_input: torch.Size([1000, 52])\n",
      "target_critic_input: 704 tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -7.6477, -1.5000, -0.0000,  0.0000,  6.7403,  5.9765, -0.0000,  0.0000,\n",
      "        -6.4677, -1.5589, 11.8004, -0.9810,  6.7403,  5.8587, 11.8004, -0.9810,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -7.7758, -1.5000,  0.0000,  0.0000, -6.7403,  5.9765,  0.0000,  0.0000,\n",
      "        -6.3379, -1.5589, 14.3793, -0.9810, -6.7403,  5.8587, 14.3793, -0.9810,\n",
      "         0.0214,  0.4292, -0.9239, -0.4335], grad_fn=<SliceBackward0>)\n",
      "Qnext shape torch.Size([1000, 1])\n",
      "Qnext 704: tensor([0.8251])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 1])\n",
      "Y 704: tensor([0.7838])\n",
      "torch.Size([1000, 4])\n",
      "Actions: 704: tensor([ 0.3933, -0.8415,  0.4793, -0.9157])\n",
      "torch.Size([1000, 52])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -7.6477, -1.5000, -0.0000,  0.0000,  6.7403,  5.9765, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -7.7758, -1.5000,  0.0000,  0.0000, -6.7403,  5.9765,  0.0000,  0.0000,\n",
      "         0.3933, -0.8415,  0.4793, -0.9157])\n",
      "torch.Size([1000, 1])\n",
      "Q 704: tensor([0.4012], grad_fn=<SliceBackward0>)\n",
      "torch.Size([])\n",
      "tensor(0.0434, grad_fn=<SmoothL1LossBackward0>)\n",
      "0.04344735\n"
     ]
    }
   ],
   "source": [
    "### DEBUG MADDPG-UPDATE\n",
    "\n",
    "from utilities import soft_update, transpose_to_tensor, transpose_list\n",
    "samples = buffer.sample(batchsize)\n",
    "obs, action, reward, next_obs, done = map(transpose_to_tensor, samples)\n",
    "\n",
    "obs_full = torch.stack(obs,dim=1)\n",
    "n = obs_full.shape[0]\n",
    "obs_full = obs_full.reshape(n,2*24)        \n",
    "next_obs_full = torch.stack(next_obs,dim=1)\n",
    "next_obs_full = next_obs_full.reshape(n,2*24)\n",
    "\n",
    "'''\n",
    "print(n)\n",
    "print(len(obs))\n",
    "print(obs[0].shape)\n",
    "print(obs[1].shape)\n",
    "print(obs[0][704,:])\n",
    "print(obs[1][704,:])\n",
    "print(obs_full[704,:])\n",
    "print('#################')\n",
    "\n",
    "print(len(next_obs))\n",
    "print(next_obs[0].shape)\n",
    "print(next_obs[1].shape)\n",
    "print(next_obs[0][704,:])\n",
    "print(next_obs[1][704,:])\n",
    "print(next_obs_full.shape)\n",
    "'''\n",
    "print(next_obs_full[704,:])\n",
    "\n",
    "\n",
    "#agent = self.maddpg_agent[agent_number]\n",
    "#agent.critic_optimizer.zero_grad()\n",
    "\n",
    "target_actions = maddpg.target_act(next_obs)\n",
    "target_actions = torch.cat(target_actions, dim=1)\n",
    "\n",
    "print('Actions:shape',target_actions.shape)\n",
    "print('Actions: 704:',target_actions[704,:])\n",
    "\n",
    "\n",
    "target_critic_input = torch.cat((next_obs_full,target_actions), dim=1)\n",
    "print('target_critic_input:',target_critic_input.shape)\n",
    "print('target_critic_input: 704',target_critic_input[704,:])\n",
    "with torch.no_grad():\n",
    "    q_next = maddpg.return_agent(0).target_critic(target_critic_input)\n",
    "print('Qnext shape',q_next.shape)\n",
    "print('Qnext 704:',q_next[704])\n",
    "\n",
    "\n",
    "discount_factor = 0.95\n",
    "agent_number = 0 \n",
    "\n",
    "print(reward[0].shape)\n",
    "print(done[agent_number].shape)\n",
    "#print((1 - done[agent_number].view(-1, 1)))\n",
    "#print(reward[agent_number].view(-1, 1))\n",
    "#print(reward[agent_number])\n",
    "y = reward[agent_number].view(-1, 1) + discount_factor * q_next * (1 - done[agent_number].view(-1, 1))\n",
    "print(y.shape)\n",
    "print('Y 704:',y[704])\n",
    "\n",
    "action = torch.cat(action, dim=1)\n",
    "print(action.shape)\n",
    "print('Actions: 704:',action[704,:])\n",
    "### OLD -> critic_input = torch.cat((obs_full.t(), action), dim=1).to(device)\n",
    "critic_input = torch.cat((obs_full, action), dim=1)\n",
    "print(critic_input.shape)\n",
    "#print(obs_full[704,:])\n",
    "print(critic_input[704,:])\n",
    "q = maddpg.return_agent(0).critic(critic_input)\n",
    "print(q.shape)\n",
    "print('Q 704:',q[704,:])\n",
    "\n",
    "huber_loss = torch.nn.SmoothL1Loss()\n",
    "critic_loss = huber_loss(q, y.detach())\n",
    "print(critic_loss.shape)\n",
    "print(critic_loss)\n",
    "print(np.mean( 0.5* (q.detach().numpy() - y.detach().numpy())**2 ))  # detach Returns a new Tensor, \n",
    "                                                                # detached from the current graph. \n",
    "                                                                # The result will never require gradient.\n",
    "## how to debug this??\n",
    "critic_loss.backward()\n",
    "maddpg.return_agent(0).critic_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000, 2])\n",
      "tensor([-0.0426,  0.1942], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.7345, -0.3538])\n",
      "torch.Size([1000, 4])\n",
      "tensor([-0.0426,  0.1942, -0.7345, -0.3538], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1000, 52])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -7.6477, -1.5000, -0.0000,  0.0000,  6.7403,  5.9765, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -7.7758, -1.5000,  0.0000,  0.0000, -6.7403,  5.9765,  0.0000,  0.0000,\n",
      "        -0.0426,  0.1942, -0.7345, -0.3538], grad_fn=<SliceBackward0>)\n",
      "tensor(-0.6452, grad_fn=<NegBackward0>)\n",
      "-0.6451784372329712\n",
      "0.043362393975257874\n"
     ]
    }
   ],
   "source": [
    "#### ACTOR \n",
    "maddpg_agents = [maddpg.return_agent(0),maddpg.return_agent(1)]\n",
    "q_input = [ maddpg_agents[i].actor(ob) if i == agent_number \\\n",
    "                   else maddpg_agents[i].actor(ob).detach()\n",
    "                   for i, ob in enumerate(obs) ]\n",
    "\n",
    "print(len(q_input))\n",
    "print(q_input[0].shape)\n",
    "print(q_input[1].shape)\n",
    "print(q_input[0][704,:])\n",
    "print(q_input[1][704,:])\n",
    "\n",
    "\n",
    "q_input = torch.cat(q_input, dim=1)\n",
    "print(q_input.shape)\n",
    "print(q_input[704,:])\n",
    "\n",
    "q_input2 = torch.cat((obs_full, q_input), dim=1)\n",
    "print(q_input2.shape)\n",
    "print(q_input2[704,:])\n",
    "print(-maddpg.return_agent(0).critic(q_input2).mean())\n",
    "\n",
    "actor_loss = -maddpg.return_agent(0).critic(q_input2).mean()\n",
    "actor_loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(maddpg.return_agent(0).actor.parameters(),0.5)   ## added this clipping... which was uncommented in orgi\n",
    "maddpg.return_agent(0).actor_optimizer.step()\n",
    "\n",
    "print(actor_loss.cpu().detach().item())\n",
    "print(critic_loss.cpu().detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True]\n",
      "######\n",
      "<class 'list'>\n",
      "5\n",
      "[[array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -6.95863676, -1.5       , -0.        ,  0.        ,\n",
      "        -6.47696209,  5.96076012, -0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -7.71691895, -1.5       ,  0.        ,  0.        ,\n",
      "         6.47696209,  5.96076012,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -6.95863676, -1.5       , -0.        ,  0.        ,\n",
      "        -6.47696209,  5.96076012, -0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -7.71691895, -1.5       ,  0.        ,  0.        ,\n",
      "         6.47696209,  5.96076012,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -6.95863676, -1.5       , -0.        ,  0.        ,\n",
      "        -6.47696209,  5.96076012, -0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -7.71691895, -1.5       ,  0.        ,  0.        ,\n",
      "         6.47696209,  5.96076012,  0.        ,  0.        ]])], [array([-0.9938956,  1.0018964], dtype=float32), array([-0.9938956,  1.0018964], dtype=float32), array([-0.9938956,  1.0018964], dtype=float32)], [[0.0, -0.009999999776482582], [0.0, -0.009999999776482582], [0.0, -0.009999999776482582]], [array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -6.95863676, -1.5       , -0.        ,  0.        ,\n",
      "        -6.47696209,  5.96076012, -0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -7.71691895, -1.5       ,  0.        ,  0.        ,\n",
      "         6.47696209,  5.96076012,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -6.95863676, -1.5       , -0.        ,  0.        ,\n",
      "        -6.47696209,  5.96076012, -0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -7.71691895, -1.5       ,  0.        ,  0.        ,\n",
      "         6.47696209,  5.96076012,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -6.95863676, -1.5       , -0.        ,  0.        ,\n",
      "        -6.47696209,  5.96076012, -0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -7.71691895, -1.5       ,  0.        ,  0.        ,\n",
      "         6.47696209,  5.96076012,  0.        ,  0.        ]])], [[True, False], [True, False], [True, False]]]\n",
      "######\n",
      "lets go\n",
      "lets go\n",
      "lets go\n",
      "lets go\n",
      "lets go\n",
      "obs: [tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,  5.9608, -0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,  5.9608, -0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,  5.9608, -0.0000,  0.0000]]), tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,  5.9608,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,  5.9608,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,  5.9608,  0.0000,  0.0000]])]\n",
      "obs_full: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,  5.9608, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,  5.9608,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,  5.9608, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,  5.9608,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,  5.9608, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,  5.9608,  0.0000,  0.0000]])\n",
      "obs_full _shape torch.Size([3, 48])\n",
      "next_obs: [tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,  5.9608, -0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,  5.9608, -0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,  5.9608, -0.0000,  0.0000]]), tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,  5.9608,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,  5.9608,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,  5.9608,  0.0000,  0.0000]])]\n",
      "next_obs_full: tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000, -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,\n",
      "           5.9608, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000, -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,\n",
      "           5.9608,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000, -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,\n",
      "           5.9608, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000, -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,\n",
      "           5.9608,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000, -6.9586, -1.5000, -0.0000,  0.0000, -6.4770,\n",
      "           5.9608, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000, -7.7169, -1.5000,  0.0000,  0.0000,  6.4770,\n",
      "           5.9608,  0.0000,  0.0000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nobs_full = torch.stack(obs_full)\\nnext_obs_full = torch.stack(next_obs_full)\\nprint('next_obs_full:',next_obs_full)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TESTING BUFFER push/pull\n",
    "#buffer = ReplayBuffer(int(5000*episode_length))  ## buffer ToDo\n",
    "\n",
    "dones = env_info.local_done                        # see if episode finished\n",
    "print(dones)\n",
    "#assert 1== 0\n",
    "dones = [True,False]\n",
    "\n",
    "def transpose_to_tensor(input_list):\n",
    "    print('lets go')\n",
    "    make_tensor = lambda x: torch.tensor(x, dtype=torch.float)\n",
    "    return list(map(make_tensor, zip(*input_list)))\n",
    "\n",
    "transition = ([states], actions_array, [rewards], [next_states], [dones])\n",
    "for jj in range(10):\n",
    "    buffer.push(transition)\n",
    "\n",
    "samples = buffer.sample(3)\n",
    "print('######')\n",
    "print(type(samples))\n",
    "print(len(samples))\n",
    "print(samples)\n",
    "print('######')\n",
    "\n",
    "'''\n",
    "print([rewards])\n",
    "#obs, obs_full, action, reward, next_obs, next_obs_full, done = map(transpose_to_tensor, samples)\n",
    "obs, action, reward, next_obs,  done = map(transpose_to_tensor, samples)\n",
    "print(reward)\n",
    "#obs, action, reward,  next_obs,  done   = map(transpose_to_tensor,(states, actions_array, [rewards], next_states, [dones]))\n",
    "'''\n",
    "\n",
    "\n",
    "obs, action, reward, next_obs,  done = map(transpose_to_tensor, samples)\n",
    "'''\n",
    "print('obs:',obs[0].shape)\n",
    "print('obs:',len(obs))\n",
    "print('rewards:',reward)\n",
    "print('dones:',done)\n",
    "'''\n",
    "print('obs:',obs)\n",
    "\n",
    "obs_full = torch.stack(obs,dim=1)\n",
    "n = obs_full.shape[0]\n",
    "obs_full = obs_full.reshape(n,2*24)\n",
    "print('obs_full:',obs_full)\n",
    "print('obs_full _shape',obs_full.shape)\n",
    "\n",
    "\n",
    "print('next_obs:',next_obs)\n",
    "next_obs_full = torch.stack(next_obs,dim=1)\n",
    "#next_obs_full = next_obs_full.reshape(n,2*24)\n",
    "print('next_obs_full:',next_obs_full)\n",
    "\n",
    "'''\n",
    "obs_full = torch.stack(obs_full)\n",
    "next_obs_full = torch.stack(next_obs_full)\n",
    "print('next_obs_full:',next_obs_full)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.68513823 -1.5\n",
      "  -0.          0.         -6.70838642  5.96076012 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.23882484 -1.5\n",
      "   0.          0.          6.70838642  5.96076012  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lets go\n",
      "lets go\n",
      "lets go\n",
      "lets go\n",
      "lets go\n"
     ]
    }
   ],
   "source": [
    "obs, action, reward, next_obs, done = map(transpose_to_tensor, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1356,  0.3300],\n",
       "         [-0.1356,  0.3300],\n",
       "         [-0.1356,  0.3300]]),\n",
       " tensor([[-0.2193,  0.3981],\n",
       "         [-0.2193,  0.3981],\n",
       "         [-0.2193,  0.3981]])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.cat(action, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2373, 2.0914, 0.2131, 2.0083],\n",
       "        [1.5938, 1.1650, 1.5151, 1.0715],\n",
       "        [1.3315, 0.2975, 1.0845, 0.1133]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
