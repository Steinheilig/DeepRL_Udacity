{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent's Performance \n",
    "# Multi-Agent Deep Deterministic Policy Gradients (MADDPG)\n",
    "---\n",
    "Notebook, addapted from \n",
    "https://raw.githubusercontent.com/udacity/deep-reinforcement-learning/master/ddpg-bipedal/DDPG.ipynb\n",
    "training DDPG with OpenAI Gym's BipedalWalker-v2 environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from maddpg import MADDPG     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='C:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_collab-compet\\Tennis_Windows_x86_64\\Tennis.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Network Weights and run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init OUNoise with dim= 2\n",
      "init OUNoise with dim= 2\n"
     ]
    }
   ],
   "source": [
    "data = np.load('state_scale.npz')\n",
    "scale = data['scale_int']\n",
    "\n",
    "# initialize policy\n",
    "maddpg = MADDPG()\n",
    "\n",
    "# load saved weights:\n",
    "save_dict_list = torch.load('.\\model_dir\\Run5_reduced_episode-6240.pt')\n",
    "for i in range(2):\n",
    "    maddpg.maddpg_agent[i].actor.load_state_dict(save_dict_list[i]['actor_params'])\n",
    "    maddpg.maddpg_agent[i].critic.load_state_dict(save_dict_list[i]['critic_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Total score (averaged over agents) this episode: 2.600000038743019\n",
      "1 Total score (averaged over agents) this episode: 2.650000039488077\n",
      "2 Total score (averaged over agents) this episode: 2.600000038743019\n",
      "3 Total score (averaged over agents) this episode: 2.600000038743019\n",
      "4 Total score (averaged over agents) this episode: 2.600000038743019\n",
      "5 Total score (averaged over agents) this episode: 2.600000038743019\n",
      "6 Total score (averaged over agents) this episode: 2.650000039488077\n",
      "7 Total score (averaged over agents) this episode: 2.600000038743019\n",
      "8 Total score (averaged over agents) this episode: 2.650000039488077\n",
      "9 Total score (averaged over agents) this episode: 2.550000037997961\n"
     ]
    }
   ],
   "source": [
    "num_agents = 2\n",
    "for runs in range(10):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    states = states/scale\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = maddpg.act(torch.from_numpy(states).unsqueeze(0).float(), noise=False)\n",
    "        actions_array = torch.stack(actions).detach().numpy().squeeze()\n",
    "        \n",
    "        env_info = env.step(actions_array)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states/scale                               # roll over states to next time step\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        #states_mean.append(states[0])\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break        \n",
    "    print(runs,'Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
