{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent's Performance \n",
    "# Multi-Agent Deep Deterministic Policy Gradients (MADDPG)\n",
    "---\n",
    "Notebook, addapted from \n",
    "https://raw.githubusercontent.com/udacity/deep-reinforcement-learning/master/ddpg-bipedal/DDPG.ipynb\n",
    "training DDPG with OpenAI Gym's BipedalWalker-v2 environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from maddpg import MADDPG     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 2\n",
      "        Number of External Brains : 2\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: GoalieBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 112\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n",
      "Unity brain name: StrikerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 112\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 6\n",
      "        Vector Action descriptions: , , , , , \n"
     ]
    }
   ],
   "source": [
    "#no_graphics=True\n",
    "no_graphics=False\n",
    "env = UnityEnvironment(file_name='C:\\EigeneLokaleDaten\\DeepRL\\Value-based-methods\\p3_Soccer\\Soccer_Windows_x86_64\\Soccer.exe',no_graphics=no_graphics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow nomucaltur: https://github.com/udacity/deep-reinforcement-learning/blob/master/p3_collab-compet/Soccer.ipynb\n",
    "# set the goalie brain\n",
    "g_brain_name = env.brain_names[0]\n",
    "g_brain = env.brains[g_brain_name]\n",
    "\n",
    "# set the striker brain\n",
    "s_brain_name = env.brain_names[1]\n",
    "s_brain = env.brains[s_brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of goalie agents: 2\n",
      "Number of striker agents: 2\n",
      "Number of goalie actions: 4\n",
      "Number of striker actions: 6\n",
      "There are 2 goalie agents. Each receives a state with length: 336\n",
      "There are 2 striker agents. Each receives a state with length: 336\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)\n",
    "\n",
    "# number of agents \n",
    "num_g_agents = len(env_info[g_brain_name].agents)\n",
    "print('Number of goalie agents:', num_g_agents)\n",
    "num_s_agents = len(env_info[s_brain_name].agents)\n",
    "print('Number of striker agents:', num_s_agents)\n",
    "\n",
    "# number of actions\n",
    "g_action_size = g_brain.vector_action_space_size\n",
    "print('Number of goalie actions:', g_action_size)\n",
    "s_action_size = s_brain.vector_action_space_size\n",
    "print('Number of striker actions:', s_action_size)\n",
    "\n",
    "# examine the state space \n",
    "g_states = env_info[g_brain_name].vector_observations\n",
    "g_state_size = g_states.shape[1]\n",
    "print('There are {} goalie agents. Each receives a state with length: {}'.format(g_states.shape[0], g_state_size))\n",
    "s_states = env_info[s_brain_name].vector_observations\n",
    "s_state_size = s_states.shape[1]\n",
    "print('There are {} striker agents. Each receives a state with length: {}'.format(s_states.shape[0], s_state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Network Weights and run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init OUNoise with dim= 1\n",
      "init OUNoise with dim= 1\n"
     ]
    }
   ],
   "source": [
    "# initialize policy\n",
    "maddpg = MADDPG()\n",
    "\n",
    "'''\n",
    "# load saved weights:\n",
    "save_dict_list = torch.load('.\\model_dir\\Run3_reduced_only_actor_episode-720.pt')\n",
    "#save_dict_list = torch.load('.\\model_dir\\Run1_reduced_only_actor_episode-1450.pt')\n",
    "for i in range(2):\n",
    "    maddpg.maddpg_agent[i].actor.load_state_dict(save_dict_list[i]['actor_params'])\n",
    "\n",
    "\n",
    "# load saved weights:\n",
    "save_dict_list1 = torch.load('.\\model_dir\\Run3_reduced_only_actor_episode-720.pt')\n",
    "save_dict_list2 = torch.load('.\\model_dir\\Run1_reduced_only_actor_episode-1450.pt')\n",
    "maddpg.maddpg_agent[0].actor.load_state_dict(save_dict_list1[0]['actor_params'])\n",
    "maddpg.maddpg_agent[1].actor.load_state_dict(save_dict_list2[1]['actor_params'])\n",
    "''' \n",
    "\n",
    "\n",
    "save_dict_list = torch.load('.\\model_dir\\StrikerOnly_Run3_reduced_only_actor_episode-1410.pt')\n",
    "#save_dict_list = torch.load('.\\model_dir\\Run1_reduced_only_actor_episode-1450.pt')\n",
    "for i in range(2):\n",
    "    maddpg.maddpg_agent[i].actor.load_state_dict(save_dict_list[i]['actor_params'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601\n",
      "0 Total score (averaged over agents) this episode: -1.001666690921411\n",
      "601\n",
      "1 Total score (averaged over agents) this episode: -1.001666690921411\n",
      "601\n",
      "2 Total score (averaged over agents) this episode: -1.001666690921411\n",
      "601\n",
      "3 Total score (averaged over agents) this episode: -1.001666690921411\n",
      "601\n",
      "4 Total score (averaged over agents) this episode: -1.001666690921411\n"
     ]
    }
   ],
   "source": [
    "#maddpg = MADDPG()\n",
    "\n",
    "num_agents = 2\n",
    "for runs in range(5):\n",
    "    #maddpg.maddpg_agent[0].actor.reset_parameters()\n",
    "    #maddpg.maddpg_agent[1].actor.reset_parameters()\n",
    "    noise = 1\n",
    "    noise_reduction = 0.999\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)      # reset the environment    \n",
    "    states_f = env_info[s_brain_name].vector_observations                  # get the current state (for each agent)\n",
    "    states = states_f[:,-224:]\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    jj = 0 \n",
    "    while True:        \n",
    "        jj+=1\n",
    "        actions = maddpg.act(torch.from_numpy(states).unsqueeze(0).float(), noise=False)\n",
    "        noise *= noise_reduction  \n",
    "        actions_array = torch.stack(actions).detach().numpy().squeeze()\n",
    "        #print(actions_array,end=\"\") \n",
    "        #print('\\rEpisode {}\\tActions <Strikers>: {:.2f} {:.2f}'.format(actions_array[0][0],actions_array[0][1]), end=\"\")                \n",
    "        \n",
    "        g_actions = np.random.randint(g_action_size, size=num_g_agents)\n",
    "        #g_actions = np.random.randint(1, size=num_g_agents)+2  # 0 -> towards center , 1 towards goal, 2 right, 3 left\n",
    "        s_actions = actions_array.squeeze() #actions_array\n",
    "        actions = dict(zip([g_brain_name, s_brain_name], \n",
    "                           [g_actions, s_actions]))\n",
    "        env_info = env.step(actions)           # send all actions to the environment\n",
    "\n",
    "        next_states_f = env_info[s_brain_name].vector_observations         # get next state (for each agent)\n",
    "        next_states = next_states_f[:,-224:]\n",
    "        rewards = env_info[s_brain_name].rewards                         # get reward (for each agent)\n",
    "        dones = env_info[s_brain_name].local_done                        # see if episode finished\n",
    "        scores += env_info[s_brain_name].rewards                         # update the score (for each agent)\n",
    "        \n",
    "        \n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break        \n",
    "    print(jj)\n",
    "    print(runs,'Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7881088379839447"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5486469074854967"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_reduction**600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9047921471137089"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.999**100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6063789448611847 0.6975506718651009 0.8606433826830363\n",
      "0.8352326125642842 0.9277260855008075\n"
     ]
    }
   ],
   "source": [
    "print(0.999**500,0.999**360,0.999**150)\n",
    "print(0.9995**360,0.9995**150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
